{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81de7a96-62a6-4af9-8baa-8fbdaca71a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] merged: labels.json  -> images=0\n",
      "[train] merged: _annotations.coco.json  -> images=4155\n",
      "[train] merged: _annotations.coco.json  -> images=2034\n",
      "[val] merged: _annotations.coco.json  -> images=307\n",
      "[val] merged: _annotations.coco.json  -> images=580\n",
      "[test] merged: _annotations.coco.json  -> images=303\n",
      "[test] merged: _annotations.coco.json  -> images=281\n",
      "✓ Wrote F:\\smart-crossing\\training\\data\\split\\train.json | images=6189 annots=0 cats=0\n",
      "✓ Wrote F:\\smart-crossing\\training\\data\\split\\val.json | images=887 annots=0 cats=0\n",
      "✓ Wrote F:\\smart-crossing\\training\\data\\split\\test.json | images=584 annots=0 cats=0\n",
      "Done. Merged dataset at: F:\\smart-crossing\\training\\data\\split\n"
     ]
    }
   ],
   "source": [
    "import os, json, shutil, re, random\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- CONFIG (edit paths only if yours differ) ----------\n",
    "ROOT = Path(r\"F:\\smart-crossing\\training\\data\")\n",
    "\n",
    "OUT_DIR = ROOT / \"split\"   # merged output here\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper to find images dir if dataset uses train/images vs train/\n",
    "def images_dir_of(split_dir: Path) -> Path:\n",
    "    return split_dir/\"images\" if (split_dir/\"images\").exists() else split_dir\n",
    "\n",
    "# Datasets to merge (COCO JSON + images dir)\n",
    "# 1) COCO subset export (all images -> train split by default)\n",
    "COCO_SUB_JSON  = ROOT / \"raw\" / \"coco_subset_export\" / \"labels.json\"\n",
    "COCO_SUB_IMGS  = ROOT / \"raw\" / \"coco_subset_export\" / \"data\"\n",
    "\n",
    "# 2) Roboflow pedestrian lights (has train/valid/test)\n",
    "PL_ROOT        = ROOT / \"raw\" / \"pedestrian-lights-1\"\n",
    "PL = {\n",
    "    \"train\": {\"json\": PL_ROOT/\"train\"/\"_annotations.coco.json\", \"imgs\": images_dir_of(PL_ROOT/\"train\")},\n",
    "    \"val\":   {\"json\": (PL_ROOT/\"valid\"/\"_annotations.coco.json\" if (PL_ROOT/\"valid\").exists() else PL_ROOT/\"val\"/\"_annotations.coco.json\"),\n",
    "              \"imgs\": images_dir_of(PL_ROOT/\"valid\" if (PL_ROOT/\"valid\").exists() else PL_ROOT/\"val\")},\n",
    "    \"test\":  {\"json\": PL_ROOT/\"test\"/\"_annotations.coco.json\",  \"imgs\": images_dir_of(PL_ROOT/\"test\")},\n",
    "}\n",
    "\n",
    "# 3) Roboflow zebra crossings (has train/valid/test)\n",
    "ZB_ROOT        = ROOT / \"raw\" / \"roboflow_zebra\"\n",
    "ZB = {\n",
    "    \"train\": {\"json\": ZB_ROOT/\"train\"/\"_annotations.coco.json\", \"imgs\": images_dir_of(ZB_ROOT/\"train\")},\n",
    "    \"val\":   {\"json\": (ZB_ROOT/\"valid\"/\"_annotations.coco.json\" if (ZB_ROOT/\"valid\").exists() else ZB_ROOT/\"val\"/\"_annotations.coco.json\"),\n",
    "              \"imgs\": images_dir_of(ZB_ROOT/\"valid\" if (ZB_ROOT/\"valid\").exists() else ZB_ROOT/\"val\")},\n",
    "    \"test\":  {\"json\": ZB_ROOT/\"test\"/\"_annotations.coco.json\",  \"imgs\": images_dir_of(ZB_ROOT/\"test\")},\n",
    "}\n",
    "\n",
    "# Put all sources here. COCO subset goes to TRAIN only.\n",
    "SOURCES = {\n",
    "    \"train\": [\n",
    "        {\"name\":\"coco_sub\", \"json\": COCO_SUB_JSON, \"imgs\": COCO_SUB_IMGS},\n",
    "        PL[\"train\"], {\"name\":\"pl_train\", **PL[\"train\"]},\n",
    "        ZB[\"train\"], {\"name\":\"zb_train\", **ZB[\"train\"]},\n",
    "    ],\n",
    "    \"val\":   [PL[\"val\"], {\"name\":\"pl_val\", **PL[\"val\"]},\n",
    "              ZB[\"val\"], {\"name\":\"zb_val\", **ZB[\"val\"]}],\n",
    "    \"test\":  [PL[\"test\"], {\"name\":\"pl_test\", **PL[\"test\"]},\n",
    "              ZB[\"test\"], {\"name\":\"zb_test\", **ZB[\"test\"]}],\n",
    "}\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "def norm_name(name: str) -> str:\n",
    "    \"\"\"Normalize category names so 'traffic light' and 'traffic_light' become same.\"\"\"\n",
    "    s = name.strip().lower()\n",
    "    s = re.sub(r\"[\\s\\-]+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "def load_coco(json_path: Path):\n",
    "    d = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "    # Ensure required keys exist\n",
    "    d.setdefault(\"images\", []); d.setdefault(\"annotations\", []); d.setdefault(\"categories\", [])\n",
    "    return d\n",
    "\n",
    "def ensure_out_split(split: str):\n",
    "    (OUT_DIR/split/\"images\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Build unified category map (by name)\n",
    "unified_name2id = {}\n",
    "unified_id2name = {}\n",
    "next_cat_id = 1\n",
    "\n",
    "def add_categories_from(d):\n",
    "    global next_cat_id\n",
    "    for c in d[\"categories\"]:\n",
    "        cname = norm_name(c[\"name\"])\n",
    "        if cname not in unified_name2id:\n",
    "            unified_name2id[cname] = next_cat_id\n",
    "            unified_id2name[next_cat_id] = c[\"name\"]  # keep original casing of first-seen\n",
    "            next_cat_id += 1\n",
    "\n",
    "# First pass: scan all sources to collect category names\n",
    "for split, items in SOURCES.items():\n",
    "    for i in range(0, len(items), 2):   # items were added as dict,dict (keep both ways)\n",
    "        src = items[i]\n",
    "        if not isinstance(src, dict) or \"json\" not in src: \n",
    "            continue\n",
    "        jp = Path(src[\"json\"])\n",
    "        if jp.exists():\n",
    "            add_categories_from(load_coco(jp))\n",
    "\n",
    "# Data holders per split\n",
    "merged = {\n",
    "    \"train\": {\"images\": [], \"annotations\": [], \"categories\": []},\n",
    "    \"val\":   {\"images\": [], \"annotations\": [], \"categories\": []},\n",
    "    \"test\":  {\"images\": [], \"annotations\": [], \"categories\": []},\n",
    "}\n",
    "for s in merged:\n",
    "    merged[s][\"categories\"] = [{\"id\": i, \"name\": unified_id2name[i]} for i in sorted(unified_id2name)]\n",
    "\n",
    "# Copy + remap\n",
    "random.seed(123)\n",
    "for split, items in SOURCES.items():\n",
    "    ensure_out_split(split)\n",
    "\n",
    "    next_img_id = 1 + len(merged[split][\"images\"])\n",
    "    next_ann_id = 1 + len(merged[split][\"annotations\"])\n",
    "\n",
    "    # Some SOURCES entries were duplicated when using dict expansion; filter valid ones\n",
    "    valid_items = []\n",
    "    for j in range(0, len(items), 2):\n",
    "        src = items[j]\n",
    "        if isinstance(src, dict) and \"json\" in src:\n",
    "            valid_items.append(src)\n",
    "\n",
    "    for idx, src in enumerate(valid_items, 1):\n",
    "        json_path = Path(src[\"json\"])\n",
    "        img_dir   = Path(src[\"imgs\"])\n",
    "        if not json_path.exists():\n",
    "            print(f\"[WARN] Missing JSON: {json_path} (skipped)\")\n",
    "            continue\n",
    "        if not img_dir.exists():\n",
    "            print(f\"[WARN] Missing images dir: {img_dir} (skipped)\")\n",
    "            continue\n",
    "\n",
    "        data = load_coco(json_path)\n",
    "        # Map source cat id -> unified cat id\n",
    "        src_id2uni = {}\n",
    "        for c in data[\"categories\"]:\n",
    "            src_id2uni[c[\"id\"]] = unified_name2id[norm_name(c[\"name\"])]\n",
    "\n",
    "        # Copy images with unique prefixed names to avoid collisions\n",
    "        ds_prefix = json_path.parent.parent.name  # e.g., 'coco_subset_export' or dataset split folder\n",
    "        ds_tag = src.get(\"name\") or ds_prefix\n",
    "\n",
    "        # Build map src image id -> new image id + filename\n",
    "        imid_map = {}\n",
    "        for im in data[\"images\"]:\n",
    "            old_fname = im[\"file_name\"]\n",
    "            src_path  = (img_dir / old_fname) if (img_dir / old_fname).exists() else img_dir / Path(old_fname).name\n",
    "            if not src_path.exists():\n",
    "                # Some COCO JSONs keep relative paths; try last part only\n",
    "                src_path = img_dir / Path(old_fname).name\n",
    "                if not src_path.exists():\n",
    "                    # Skip missing image\n",
    "                    continue\n",
    "\n",
    "            new_fname = f\"{ds_tag}__{old_fname}\"\n",
    "            dst_path  = OUT_DIR / split / \"images\" / new_fname\n",
    "            if not dst_path.exists():\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "\n",
    "            merged[split][\"images\"].append({\n",
    "                \"id\": next_img_id,\n",
    "                \"file_name\": new_fname,\n",
    "                \"width\": im.get(\"width\"),\n",
    "                \"height\": im.get(\"height\"),\n",
    "            })\n",
    "            imid_map[im[\"id\"]] = next_img_id\n",
    "            next_img_id += 1\n",
    "\n",
    "        # Remap annotations to new ids\n",
    "        for ann in data[\"annotations\"]:\n",
    "            sid = ann.get(\"image_id\")\n",
    "            if sid not in imid_map:\n",
    "                continue  # image was missing\n",
    "            merged[split][\"annotations\"].append({\n",
    "                \"id\": next_ann_id,\n",
    "                \"image_id\": imid_map[sid],\n",
    "                \"category_id\": src_id2uni[ann[\"category_id\"]],\n",
    "                \"bbox\": ann.get(\"bbox\", []),\n",
    "                \"area\": float(ann.get(\"area\", 0)),\n",
    "                \"iscrowd\": int(ann.get(\"iscrowd\", 0)),\n",
    "                \"segmentation\": ann.get(\"segmentation\", []),\n",
    "            })\n",
    "            next_ann_id += 1\n",
    "\n",
    "        print(f\"[{split}] merged: {json_path.name}  -> images={len(imid_map)}\")\n",
    "\n",
    "# Write COCO JSONs\n",
    "for split in [\"train\",\"val\",\"test\"]:\n",
    "    out_json = OUT_DIR / (f\"{split}.json\")\n",
    "    with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(merged[split], f)\n",
    "    print(f\"✓ Wrote {out_json} | images={len(merged[split]['images'])} annots={len(merged[split]['annotations'])} cats={len(merged[split]['categories'])}\")\n",
    "\n",
    "print(\"Done. Merged dataset at:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python (sca)",
   "language": "python",
   "name": "sca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
